# ğŸ“Š Etapas del Pipeline - Flujo Completo

## ğŸ“‹ DescripciÃ³n

Este documento describe en detalle cada etapa del pipeline de datos, desde la ingesta inicial hasta la generaciÃ³n de documentaciÃ³n, incluyendo el script de procesamiento Python.

## ğŸ”„ Flujo Completo del Pipeline

### 1ï¸âƒ£ **Etapa de Ingesta (RAW) - Script Python**
```
ğŸ“¥ Input: Data Engineer Challenge.xlsx
    â†“
ğŸ”§ Procesamiento: ingest_excel_to_duckdb.py
    â†“
ğŸ“Š Outputs:
   â”œâ”€â”€ raw_data/raw_loans_<hash>.parquet
   â”œâ”€â”€ dbt/data_challenge.duckdb (raw.raw_loans) â† TABLA RAW CREADA AQUÃ
   â””â”€â”€ s3://bucket/raw/loans/ (modo producciÃ³n)
```

**âš ï¸ IMPORTANTE: La capa RAW se crea desde el script Python, NO desde dbt**

**Responsabilidades del Script Python:**
- âœ… Lectura del archivo Excel
- âœ… ValidaciÃ³n de datos fuente
- âœ… Escritura en formato Parquet
- âœ… **CreaciÃ³n de la tabla `raw.raw_loans` en DuckDB**
- âœ… Upload opcional a S3

**â˜ï¸ PRODUCCIÃ“N vs DESARROLLO:**
- **Desarrollo (Actual)**: RAW almacenado en DuckDB local
- **ProducciÃ³n (AWS)**: RAW deberÃ­a almacenarse en S3 buckets
- **MigraciÃ³n**: Script incluye funcionalidad S3 para producciÃ³n

### 2ï¸âƒ£ **Etapa de TransformaciÃ³n (SILVER) - dbt**
```
ğŸ“¥ Input: raw.raw_loans (creada por el script Python)
    â†“
ğŸ”§ Procesamiento: dbt run --select silver_loans
    â†“
ğŸ“Š Output: silver.silver_loans
```

**Responsabilidades de dbt:**
- âœ… Limpieza de datos
- âœ… DeduplicaciÃ³n (18% de registros removidos)
- âœ… NormalizaciÃ³n de formatos
- âœ… ValidaciÃ³n de tipos de datos
- âœ… EstandarizaciÃ³n de valores

### 3ï¸âƒ£ **Etapa de Modelado (GOLD) - dbt**
```
ğŸ“¥ Input: silver.silver_loans
    â†“
ğŸ”§ Procesamiento: dbt run --select gold:*
    â†“
ğŸ“Š Outputs:
   â”œâ”€â”€ gold.fact_loan
   â”œâ”€â”€ gold.dim_customer
   â””â”€â”€ gold.dim_purpose
```

**Responsabilidades de dbt:**
- âœ… CreaciÃ³n de modelo dimensional
- âœ… SeparaciÃ³n de hechos y dimensiones
- âœ… OptimizaciÃ³n para analytics
- âœ… GeneraciÃ³n de claves Ãºnicas

### 4ï¸âƒ£ **Etapa de ValidaciÃ³n - dbt**
```
ğŸ“¥ Input: Todas las tablas GOLD
    â†“
ğŸ”§ Procesamiento: dbt test
    â†“
ğŸ“Š Output: Reporte de validaciÃ³n
```

**Responsabilidades:**
- âœ… Tests de integridad referencial
- âœ… ValidaciÃ³n de rangos de valores
- âœ… VerificaciÃ³n de unicidad
- âœ… Checks de completitud

### 5ï¸âƒ£ **Etapa de DocumentaciÃ³n - dbt**
```
ğŸ“¥ Input: Modelos dbt
    â†“
ğŸ”§ Procesamiento: dbt docs generate
    â†“
ğŸ“Š Output: DocumentaciÃ³n HTML
```

**Responsabilidades:**
- âœ… GeneraciÃ³n de documentaciÃ³n automÃ¡tica
- âœ… Lineage de datos
- âœ… DescripciÃ³n de modelos
- âœ… MÃ©tricas de calidad

## ğŸ”§ Script de Procesamiento Python - CREADOR DE RAW

### ğŸ“‹ DescripciÃ³n

El script `ingest_excel_to_duckdb.py` es el **creador de la capa RAW**, responsable de ingerir los datos del archivo Excel y cargarlos en la tabla `raw.raw_loans` en DuckDB.

### ğŸ¯ Funcionalidades

#### ğŸ“Š Procesamiento de Datos
- âœ… **Lectura de Excel**: Pandas para procesar archivos .xlsx
- âœ… **Escritura Parquet**: Almacenamiento eficiente en formato columnar
- âœ… **Carga DuckDB**: Base de datos embebida para procesamiento
- âœ… **S3 Upload**: Opcional para entornos de producciÃ³n

#### ğŸ”„ Flujo de Procesamiento

```
Excel File â†’ Pandas DataFrame â†’ Parquet Files â†’ DuckDB Table (raw.raw_loans)
     â†“              â†“              â†“              â†“
  Raw Data    â†’  Processing   â†’  Storage    â†’  Database
```

### ğŸ“ Archivo Principal

#### `ingest/ingest_excel_to_duckdb.py`
```python
#!/usr/bin/env python3
"""
Script de ingesta: Excel â†’ RAW (Parquet + DuckDB)
CREADOR DE LA CAPA RAW - Primer paso del pipeline de datos de prÃ©stamos.
"""

import argparse
import hashlib
import os
import sys
from pathlib import Path
from typing import Optional

import duckdb
import pandas as pd
import boto3
from botocore.exceptions import ClientError

def read_excel_data(excel_path: Path) -> pd.DataFrame:
    """Lee el archivo Excel y retorna un DataFrame limpio"""
    try:
        print(f"ğŸ“– Leyendo Excel: {excel_path}")
        df = pd.read_excel(excel_path)
        print(f"âœ… Excel leÃ­do: {len(df)} filas, {len(df.columns)} columnas")
        return df
    except Exception as e:
        raise FileNotFoundError(f"Error leyendo Excel: {e}")

def write_parquet(df: pd.DataFrame, raw_dir: Path) -> Path:
    """Escribe el DataFrame como archivo Parquet"""
    try:
        raw_dir.mkdir(parents=True, exist_ok=True)
        
        # Generar nombre Ãºnico basado en contenido
        content_hash = hashlib.md5(df.to_string().encode()).hexdigest()
        parquet_path = raw_dir / f"raw_loans_{content_hash}.parquet"
        
        df.to_parquet(parquet_path, index=False)
        print(f"âœ… RAW Parquet escrito: {parquet_path}")
        return parquet_path
    except Exception as e:
        raise RuntimeError(f"Error escribiendo Parquet: {e}")

def load_to_duckdb(df: pd.DataFrame, duckdb_path: Path) -> None:
    """Carga el DataFrame a DuckDB - CREA LA TABLA RAW"""
    try:
        # Crear directorio si no existe
        duckdb_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Conectar a DuckDB
        con = duckdb.connect(str(duckdb_path))
        
        # Crear esquema raw si no existe
        con.execute("CREATE SCHEMA IF NOT EXISTS raw")
        
        # Cargar datos - ESTO CREA LA TABLA raw.raw_loans
        con.execute("DROP TABLE IF EXISTS raw.raw_loans")
        con.execute("CREATE TABLE raw.raw_loans AS SELECT * FROM df")
        
        # Verificar carga
        count = con.execute("SELECT COUNT(*) FROM raw.raw_loans").fetchone()[0]
        print(f"âœ… Ingestado en DuckDB â†’ tabla raw.raw_loans en {duckdb_path}")
        print(f"   Registros cargados: {count}")
        
        con.close()
    except Exception as e:
        raise RuntimeError(f"Error cargando a DuckDB: {e}")

def upload_to_s3(parquet_path: Path, bucket: str, prefix: str) -> None:
    """Sube el archivo Parquet a S3 (modo producciÃ³n)"""
    try:
        s3_client = boto3.client('s3')
        
        # Generar clave S3
        s3_key = f"{prefix}/{parquet_path.name}"
        
        # Subir archivo
        s3_client.upload_file(str(parquet_path), bucket, s3_key)
        print(f"âœ… Subido a S3: s3://{bucket}/{s3_key}")
    except ClientError as e:
        raise RuntimeError(f"Error subiendo a S3: {e}")

def main():
    """FunciÃ³n principal del script de ingesta - CREADOR DE RAW"""
    parser = argparse.ArgumentParser(description='Ingesta Excel â†’ RAW (Parquet + DuckDB)')
    parser.add_argument('--excel', required=True, type=Path, help='Ruta al archivo Excel')
    parser.add_argument('--duckdb', required=True, type=Path, help='Ruta a DuckDB')
    parser.add_argument('--raw_dir', required=True, type=Path, help='Directorio RAW (Parquet)')
    parser.add_argument('--prod', action='store_true', help='Modo producciÃ³n (S3)')
    parser.add_argument('--s3_bucket', help='Bucket S3')
    parser.add_argument('--s3_prefix', default='raw/loans', help='Prefijo S3')
    
    args = parser.parse_args()
    
    # Validar archivo Excel
    if not args.excel.exists():
        raise FileNotFoundError(f"No existe el Excel: {args.excel}")
    
    print("ğŸš€ Iniciando ingesta Excel â†’ RAW")
    print("=" * 50)
    
    # 1. Leer Excel
    df = read_excel_data(args.excel)
    
    # 2. Escribir Parquet
    parquet_path = write_parquet(df, args.raw_dir)
    
    # 3. Cargar DuckDB - CREA LA TABLA raw.raw_loans
    load_to_duckdb(df, args.duckdb)
    
    # 4. Subir S3 (si modo producciÃ³n)
    if args.prod:
        if not args.s3_bucket:
            raise ValueError("--prod requiere --s3_bucket")
        upload_to_s3(parquet_path, args.s3_bucket, args.s3_prefix)
    
    print("=" * 50)
    print("âœ… Ingesta completada exitosamente")
    print("ğŸ“Š Tabla raw.raw_loans creada y lista para dbt")

if __name__ == '__main__':
    main()
```

### ğŸ”§ ConfiguraciÃ³n y Uso

#### ğŸ“‹ Argumentos del Script
```bash
python ingest_excel_to_duckdb.py \
    --excel "Data Engineer Challenge.xlsx" \
    --duckdb "dbt/data_challenge.duckdb" \
    --raw_dir "raw_data"
```

#### ğŸš€ Modo ProducciÃ³n (S3)
```bash
python ingest_excel_to_duckdb.py \
    --excel "Data Engineer Challenge.xlsx" \
    --duckdb "dbt/data_challenge.duckdb" \
    --raw_dir "raw_data" \
    --prod \
    --s3_bucket "my-raw-data-bucket" \
    --s3_prefix "raw/loans"
```

#### ğŸ“Š Salidas del Script
- **Parquet Files**: `raw_data/raw_loans_<hash>.parquet`
- **DuckDB Table**: `raw.raw_loans` â† **TABLA RAW CREADA AQUÃ**
- **S3 Files**: `s3://bucket/prefix/raw_loans_<hash>.parquet` (modo producciÃ³n)

## ğŸ“ˆ MÃ©tricas del Pipeline

### â±ï¸ Tiempos de EjecuciÃ³n
- **Ingesta (Python)**: ~30 segundos
- **TransformaciÃ³n SILVER (dbt)**: ~0.2 segundos
- **TransformaciÃ³n GOLD (dbt)**: ~0.4 segundos
- **Tests**: ~0.1 segundos
- **Total**: ~1 minuto

### ğŸ“Š VolÃºmenes de Datos
- **Entrada**: 100,000 registros
- **SILVER**: 81,999 registros (18% deduplicaciÃ³n)
- **GOLD**: 81,999 registros + dimensiones

### ğŸ¯ Calidad de Datos
- **Tests pasados**: 5/5 (100%)
- **DeduplicaciÃ³n**: 18,001 registros removidos
- **ValidaciÃ³n**: Todos los rangos correctos

## ğŸ”„ OrquestaciÃ³n con Airflow

### ğŸ“Š Estructura del DAG
```
loan_data_pipeline
â”œâ”€â”€ ingest_excel_to_raw     # Etapa 1: IngestiÃ³n (CREA RAW)
â”œâ”€â”€ dbt_deps                # Etapa 2: Dependencias
â”œâ”€â”€ dbt_run_models          # Etapa 3: Transformaciones (SILVER + GOLD)
â”œâ”€â”€ dbt_run_tests           # Etapa 4: ValidaciÃ³n
â”œâ”€â”€ validate_data_quality   # Etapa 5: Checks adicionales
â””â”€â”€ dbt_generate_docs       # Etapa 6: DocumentaciÃ³n
```

### â° ProgramaciÃ³n
- **Frecuencia**: Diaria
- **Hora**: Configurable
- **Retry**: AutomÃ¡tico en fallos
- **Timeout**: Configurable por tarea

## ğŸš€ Optimizaciones Implementadas

### ğŸ“Š **Performance**
- **Parquet**: Formato columnar eficiente
- **DuckDB**: Base de datos embebida rÃ¡pida
- **DeduplicaciÃ³n**: EliminaciÃ³n temprana de duplicados
- **Ãndices**: AutomÃ¡ticos en DuckDB

### ğŸ” **Calidad**
- **ValidaciÃ³n**: Tests automÃ¡ticos en cada etapa
- **Logging**: Logs detallados para debugging
- **Checks**: ValidaciÃ³n de conteos y rangos
- **DocumentaciÃ³n**: GeneraciÃ³n automÃ¡tica

### ğŸ›¡ï¸ **Robustez**
- **Error Handling**: Manejo de errores en cada funciÃ³n
- **Retry Logic**: Reintentos automÃ¡ticos en Airflow
- **Validation**: VerificaciÃ³n de datos en cada paso
- **Monitoring**: Monitoreo en tiempo real

## ğŸ”§ Comandos de EjecuciÃ³n

### ğŸ“Š Pipeline Completo
```bash
# OpciÃ³n 1: Script local
python scripts/run_pipeline.py

# OpciÃ³n 2: Airflow
airflow dags trigger loan_data_pipeline

# OpciÃ³n 3: Etapas individuales
python ingest/ingest_excel_to_duckdb.py --excel "Data Engineer Challenge.xlsx" --duckdb "dbt/data_challenge.duckdb" --raw_dir "raw_data"
cd dbt && dbt run
cd dbt && dbt test
```

### ğŸ” Monitoreo
```bash
# Logs de Airflow
tail -f airflow/logs/dag_id/task_id/run_id/task_id.log

# Logs de dbt
cd dbt && dbt run --log-level debug

# Explorar base de datos
python scripts/explore_db.py
```

## ğŸ¯ Resultados Finales

### ğŸ“Š **Base de Datos Final**
- **Esquemas**: `raw`, `silver`, `gold`
- **Tablas**: 5 tablas principales
- **Registros**: 81,999 registros limpios
- **Dimensiones**: 2 tablas de dimensiones

### ğŸ“ˆ **MÃ©tricas de Calidad**
- **DeduplicaciÃ³n**: 18% de registros removidos
- **Tests**: 100% de tests pasando
- **ValidaciÃ³n**: Todos los rangos correctos
- **DocumentaciÃ³n**: Completa y actualizada

### ğŸš€ **Performance**
- **Tiempo total**: ~1 minuto
- **Escalabilidad**: Preparado para volÃºmenes mayores
- **Monitoreo**: En tiempo real
- **Mantenimiento**: Automatizado

## âš ï¸ **Puntos Clave a Recordar**

### ğŸ”´ **RAW se crea desde Python, NO desde dbt**
- El script `ingest_excel_to_duckdb.py` crea la tabla `raw.raw_loans`
- dbt solo lee desde `raw.raw_loans` para crear SILVER y GOLD
- No hay modelos dbt en la carpeta `raw/` porque no los necesitamos
- **â˜ï¸ PRODUCCIÃ“N**: En AWS, el script deberÃ­a subir datos a S3 en lugar de DuckDB local

### ğŸŸ¡ **SILVER y GOLD se crean desde dbt**
- `silver/silver_loans.sql` transforma `raw.raw_loans`
- `gold/*.sql` transforma `silver.silver_loans`

### ğŸŸ¢ **Flujo de Dependencias**
```
Python Script â†’ raw.raw_loans â†’ dbt silver â†’ dbt gold
```

---

*DocumentaciÃ³n de Etapas del Pipeline - Pipeline de PrÃ©stamos v1.0*
