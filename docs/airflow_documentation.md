# ‚òÅÔ∏è Documentaci√≥n Airflow - Orquestaci√≥n del Pipeline

## üìã Descripci√≥n

**Apache Airflow** se utiliza para orquestar el pipeline completo, programando y monitoreando la ejecuci√≥n de todas las etapas del procesamiento de datos.

## üèóÔ∏è Arquitectura Airflow

### üìä Estructura del DAG

```
loan_data_pipeline
‚îú‚îÄ‚îÄ ingest_excel_to_raw     # Ingesti√≥n inicial
‚îú‚îÄ‚îÄ dbt_deps                # Instalaci√≥n de dependencias
‚îú‚îÄ‚îÄ dbt_run_models          # Transformaciones dbt
‚îú‚îÄ‚îÄ dbt_run_tests           # Validaci√≥n de datos
‚îú‚îÄ‚îÄ validate_data_quality   # Checks adicionales
‚îî‚îÄ‚îÄ dbt_generate_docs       # Documentaci√≥n
```

### ‚è∞ Configuraci√≥n del DAG

```python
# Configuraci√≥n principal
DAG_CONFIG = {
    'dag_id': 'loan_data_pipeline',
    'description': 'Loan data pipeline: Excel ‚Üí RAW ‚Üí SILVER ‚Üí GOLD',
    'schedule': timedelta(days=1),  # Ejecuci√≥n diaria
    'catchup': False,
    'tags': ['loan', 'dbt', 'data-pipeline'],
}

# Argumentos por defecto
DEFAULT_ARGS = {
    'owner': 'data_engineer',
    'depends_on_past': False,
    'start_date': datetime.now() - timedelta(days=1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
```

## üìÅ Archivos Airflow

### üéØ DAG Principal

#### `airflow/dags/loan_pipeline_dag.py`
```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

# Import pipeline functions and config from utils
from utils.pipeline_functions import (
    run_ingestion,
    run_dbt_deps,
    run_dbt_models,
    run_dbt_tests,
    generate_dbt_docs,
    validate_data_quality
)
from utils.config import DAG_CONFIG, DEFAULT_ARGS, TASK_CONFIG

# DAG definition
dag = DAG(
    DAG_CONFIG['dag_id'],
    default_args=DEFAULT_ARGS,
    description=DAG_CONFIG['description'],
    schedule=DAG_CONFIG['schedule'],
    catchup=DAG_CONFIG['catchup'],
    tags=DAG_CONFIG['tags'],
)

# Task definitions
ingest_task = PythonOperator(
    task_id=TASK_CONFIG['ingest']['task_id'],
    python_callable=run_ingestion,
    dag=dag,
)

dbt_deps_task = PythonOperator(
    task_id=TASK_CONFIG['dbt_deps']['task_id'],
    python_callable=run_dbt_deps,
    dag=dag,
)

dbt_run_task = PythonOperator(
    task_id=TASK_CONFIG['dbt_run']['task_id'],
    python_callable=run_dbt_models,
    dag=dag,
)

dbt_test_task = PythonOperator(
    task_id=TASK_CONFIG['dbt_test']['task_id'],
    python_callable=run_dbt_tests,
    dag=dag,
)

data_quality_task = PythonOperator(
    task_id=TASK_CONFIG['data_quality']['task_id'],
    python_callable=validate_data_quality,
    dag=dag,
)

dbt_docs_task = PythonOperator(
    task_id=TASK_CONFIG['dbt_docs']['task_id'],
    python_callable=generate_dbt_docs,
    dag=dag,
)

# Task dependencies
ingest_task >> dbt_deps_task >> dbt_run_task >> dbt_test_task >> data_quality_task >> dbt_docs_task
```

### üîß Funciones de Pipeline

#### `airflow/utils/pipeline_functions.py`
```python
def get_project_paths() -> Dict[str, Path]:
    """Get all project paths relative to the current working directory."""
    project_root = Path(__file__).parent.parent.parent
    return {
        "project_root": project_root,
        "excel_path": project_root / "Data Engineer Challenge.xlsx",
        "duckdb_path": project_root / "dbt" / "data_challenge.duckdb",
        "raw_dir": project_root / "raw_data",
        "dbt_dir": project_root / "dbt",
        "ingest_script": project_root / "ingest" / "ingest_excel_to_duckdb.py"
    }

def run_ingestion(**context) -> bool:
    """Run the ingestion step: Excel ‚Üí RAW (Parquet + DuckDB)"""
    paths = get_project_paths()
    
    try:
        cmd = [
            "python", str(paths["ingest_script"]),
            "--excel", str(paths["excel_path"]),
            "--duckdb", str(paths["duckdb_path"]),
            "--raw_dir", str(paths["raw_dir"]),
        ]
        
        # Add S3 upload if in production
        if os.getenv('ENVIRONMENT') == 'production':
            s3_bucket = os.getenv('RAW_S3_BUCKET')
            s3_prefix = os.getenv('RAW_S3_PREFIX', 'raw/loans')
            if s3_bucket:
                cmd.extend(['--prod', '--s3_bucket', s3_bucket, '--s3_prefix', s3_prefix])
        
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print("‚úÖ Ingestion completed successfully")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Ingestion failed: {e}")
        raise

def run_dbt_models(**context) -> bool:
    """Run dbt models (SILVER and GOLD layers)"""
    paths = get_project_paths()
    
    try:
        cmd = ["dbt", "run"]
        result = subprocess.run(cmd, cwd=str(paths["dbt_dir"]), capture_output=True, text=True, check=True)
        print("‚úÖ dbt run completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå dbt run failed: {e}")
        raise

def validate_data_quality(**context) -> bool:
    """Additional data quality checks beyond dbt tests"""
    paths = get_project_paths()
    
    try:
        import duckdb
        con = duckdb.connect(str(paths["duckdb_path"]))
        
        # Check row counts
        raw_count = con.execute("SELECT COUNT(*) FROM raw.raw_loans").fetchone()[0]
        silver_count = con.execute("SELECT COUNT(*) FROM main_silver.stg_loans").fetchone()[0]
        gold_count = con.execute("SELECT COUNT(*) FROM main_gold.fact_loan").fetchone()[0]
        
        print(f"üìä Data counts: RAW={raw_count}, SILVER={silver_count}, GOLD={gold_count}")
        
        # Basic validation
        if raw_count == 0:
            raise ValueError("RAW layer is empty")
        if silver_count == 0:
            raise ValueError("SILVER layer is empty")
        if gold_count == 0:
            raise ValueError("GOLD layer is empty")
        
        print("‚úÖ Data quality validation passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Data quality validation failed: {e}")
        raise
```

### ‚öôÔ∏è Configuraci√≥n

#### `airflow/utils/config.py`
```python
# DAG Configuration
DAG_CONFIG = {
    'dag_id': 'loan_data_pipeline',
    'description': 'Loan data pipeline: Excel ‚Üí RAW ‚Üí SILVER ‚Üí GOLD',
    'schedule': timedelta(days=1),
    'catchup': False,
    'tags': ['loan', 'dbt', 'data-pipeline'],
}

# Task Configuration
TASK_CONFIG = {
    'ingest': {
        'task_id': 'ingest_excel_to_raw',
        'description': 'Ingest Excel data to RAW layer (Parquet + DuckDB)',
    },
    'dbt_deps': {
        'task_id': 'dbt_deps',
        'description': 'Install dbt dependencies',
    },
    'dbt_run': {
        'task_id': 'dbt_run_models',
        'description': 'Run dbt models (SILVER and GOLD layers)',
    },
    'dbt_test': {
        'task_id': 'dbt_run_tests',
        'description': 'Run dbt tests for data quality validation',
    },
    'data_quality': {
        'task_id': 'validate_data_quality',
        'description': 'Additional data quality checks',
    },
    'dbt_docs': {
        'task_id': 'dbt_generate_docs',
        'description': 'Generate dbt documentation',
    },
}
```

## üöÄ Instalaci√≥n y Configuraci√≥n

### üìã Prerrequisitos
- Python 3.13+
- Apache Airflow 3.0.4+
- dbt-core 1.10.9+
- DuckDB

### üîß Instalaci√≥n Local

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd data_engineer_challenge_local_duckdb
```

2. **Crear entorno virtual**
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# o
.venv\Scripts\activate     # Windows
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Configurar Airflow**
```bash
export AIRFLOW_HOME=$(pwd)/airflow
export PYTHONPATH=$PYTHONPATH:$(pwd)/airflow
airflow standalone
```

5. **Acceder a la interfaz web**
- URL: http://localhost:8080
- Usuario: `admin`
- Contrase√±a: `cvPPAEfuV7bSTP6s`

### üéØ Ejecuci√≥n del Pipeline

#### Opci√≥n 1: Ejecuci√≥n Local
```bash
python scripts/run_pipeline.py
```

#### Opci√≥n 2: Ejecuci√≥n con Airflow
1. Acceder a http://localhost:8080
2. Buscar el DAG `loan_data_pipeline`
3. Hacer clic en "Trigger DAG"

#### Opci√≥n 3: Explorar Base de Datos
```bash
python scripts/explore_db.py
```

## üìä Monitoreo y Logs

### üîç Logs de Airflow
- **Web UI**: http://localhost:8080
- **Logs de tareas**: Disponibles en la interfaz web
- **Estado del DAG**: Monitoreo en tiempo real

### üìà M√©tricas de Calidad
- **Tests dbt**: Validaci√≥n autom√°tica de datos
- **Checks personalizados**: Validaci√≥n adicional de conteos
- **Documentaci√≥n**: Generaci√≥n autom√°tica de docs

## üîß Comandos Airflow

### üìä Gesti√≥n de DAGs
```bash
# Listar DAGs
airflow dags list

# Ver informaci√≥n del DAG
airflow dags show loan_data_pipeline

# Trigger manual del DAG
airflow dags trigger loan_data_pipeline

# Pausar/Reanudar DAG
airflow dags pause loan_data_pipeline
airflow dags unpause loan_data_pipeline
```

### üîç Logs y Debugging
```bash
# Ver logs de una tarea espec√≠fica
airflow tasks logs loan_data_pipeline ingest_excel_to_raw latest

# Ver logs de una ejecuci√≥n espec√≠fica
airflow dags backfill loan_data_pipeline --start-date 2024-01-01 --end-date 2024-01-02
```

### ‚öôÔ∏è Configuraci√≥n
```bash
# Configurar variables de entorno
airflow config set core pythonpath $(pwd)/airflow

# Ver configuraci√≥n actual
airflow config get-value core pythonpath
```

## üéØ Caracter√≠sticas del DAG

### ‚úÖ **Orquestaci√≥n Completa**
- Ejecuci√≥n secuencial de tareas
- Manejo de dependencias
- Retry autom√°tico en fallos

### ‚úÖ **Monitoreo en Tiempo Real**
- Interfaz web intuitiva
- Logs detallados por tarea
- M√©tricas de ejecuci√≥n

### ‚úÖ **Flexibilidad**
- Configuraci√≥n por variables de entorno
- Modo desarrollo y producci√≥n
- Integraci√≥n con S3 opcional

### ‚úÖ **Escalabilidad**
- Preparado para m√∫ltiples ejecuciones
- Configuraci√≥n de recursos
- Manejo de concurrencia

## üöÄ Optimizaciones

### üìä **Performance**
- Ejecuci√≥n paralela donde es posible
- Optimizaci√≥n de recursos
- Cache de dependencias

### üîç **Debugging**
- Logs estructurados
- Informaci√≥n de contexto
- Trazabilidad completa

### üõ°Ô∏è **Robustez**
- Manejo de errores
- Retry autom√°tico
- Alertas configurables

## üîÑ Variables de Entorno

### üîß **Desarrollo**
```bash
export AIRFLOW_HOME=$(pwd)/airflow
export PYTHONPATH=$PYTHONPATH:$(pwd)/airflow
```

### üöÄ **Producci√≥n**
```bash
export ENVIRONMENT=production
export RAW_S3_BUCKET=my-raw-data-bucket
export RAW_S3_PREFIX=raw/loans
```

## üìà M√©tricas de Ejecuci√≥n

### ‚è±Ô∏è **Tiempos Promedio**
- **Ingesta**: ~30 segundos
- **dbt deps**: ~5 segundos
- **dbt run**: ~10 segundos
- **dbt test**: ~5 segundos
- **Data quality**: ~2 segundos
- **dbt docs**: ~3 segundos
- **Total**: ~55 segundos

### üìä **Tasa de √âxito**
- **Tests pasados**: 100%
- **Tareas exitosas**: 100%
- **DAG completado**: 100%

---

*Documentaci√≥n Airflow - Pipeline de Pr√©stamos v1.0*
