# â˜ï¸ DocumentaciÃ³n Airflow - OrquestaciÃ³n del Pipeline

## ğŸ“‹ DescripciÃ³n

**Apache Airflow** se utiliza para orquestar el pipeline completo, programando y monitoreando la ejecuciÃ³n de todas las etapas del procesamiento de datos.

## ğŸ—ï¸ Arquitectura Airflow

### ğŸ“Š Estructura del DAG

```
loan_data_pipeline
â”œâ”€â”€ ingest_excel_to_raw     # IngestiÃ³n inicial
â”œâ”€â”€ dbt_deps                # InstalaciÃ³n de dependencias
â”œâ”€â”€ dbt_run_models          # Transformaciones dbt
â”œâ”€â”€ dbt_run_tests           # ValidaciÃ³n de datos
â”œâ”€â”€ validate_data_quality   # Checks adicionales
â””â”€â”€ dbt_generate_docs       # DocumentaciÃ³n
```

### â° ConfiguraciÃ³n del DAG

```python
# ConfiguraciÃ³n principal
DAG_CONFIG = {
    'dag_id': 'loan_data_pipeline',
    'description': 'Loan data pipeline: Excel â†’ RAW â†’ SILVER â†’ GOLD',
    'schedule': timedelta(days=1),  # EjecuciÃ³n diaria
    'catchup': False,
    'tags': ['loan', 'dbt', 'data-pipeline'],
}

# Argumentos por defecto
DEFAULT_ARGS = {
    'owner': 'data_engineer',
    'depends_on_past': False,
    'start_date': datetime.now() - timedelta(days=1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
```

## ğŸ“ Archivos Airflow

### ğŸ¯ DAG Principal

#### `airflow/dags/loan_pipeline_dag.py`
```python
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

# Import pipeline functions and config from utils
from utils.pipeline_functions import (
    run_ingestion,
    run_dbt_deps,
    run_dbt_models,
    run_dbt_tests,
    generate_dbt_docs,
    validate_data_quality
)
from utils.config import DAG_CONFIG, DEFAULT_ARGS, TASK_CONFIG

# DAG definition
dag = DAG(
    DAG_CONFIG['dag_id'],
    default_args=DEFAULT_ARGS,
    description=DAG_CONFIG['description'],
    schedule=DAG_CONFIG['schedule'],
    catchup=DAG_CONFIG['catchup'],
    tags=DAG_CONFIG['tags'],
)

# Task definitions
ingest_task = PythonOperator(
    task_id=TASK_CONFIG['ingest']['task_id'],
    python_callable=run_ingestion,
    dag=dag,
)

dbt_deps_task = PythonOperator(
    task_id=TASK_CONFIG['dbt_deps']['task_id'],
    python_callable=run_dbt_deps,
    dag=dag,
)

dbt_run_task = PythonOperator(
    task_id=TASK_CONFIG['dbt_run']['task_id'],
    python_callable=run_dbt_models,
    dag=dag,
)

dbt_test_task = PythonOperator(
    task_id=TASK_CONFIG['dbt_test']['task_id'],
    python_callable=run_dbt_tests,
    dag=dag,
)

data_quality_task = PythonOperator(
    task_id=TASK_CONFIG['data_quality']['task_id'],
    python_callable=validate_data_quality,
    dag=dag,
)

dbt_docs_task = PythonOperator(
    task_id=TASK_CONFIG['dbt_docs']['task_id'],
    python_callable=generate_dbt_docs,
    dag=dag,
)

# Task dependencies
ingest_task >> dbt_deps_task >> dbt_run_task >> dbt_test_task >> data_quality_task >> dbt_docs_task
```

### ğŸ”§ Funciones de Pipeline

#### `airflow/utils/pipeline_functions.py`
```python
def get_project_paths() -> Dict[str, Path]:
    """Get all project paths relative to the current working directory."""
    project_root = Path(__file__).parent.parent.parent
    return {
        "project_root": project_root,
        "excel_path": project_root / "Data Engineer Challenge.xlsx",
        "duckdb_path": project_root / "dbt" / "data_challenge.duckdb",
        "raw_dir": project_root / "raw_data",
        "dbt_dir": project_root / "dbt",
        "ingest_script": project_root / "ingest" / "ingest_excel_to_duckdb.py"
    }

def run_ingestion(**context) -> bool:
    """Run the ingestion step: Excel â†’ RAW (Parquet + DuckDB)"""
    paths = get_project_paths()
    
    try:
        cmd = [
            "python", str(paths["ingest_script"]),
            "--excel", str(paths["excel_path"]),
            "--duckdb", str(paths["duckdb_path"]),
            "--raw_dir", str(paths["raw_dir"]),
        ]
        
        # Add S3 upload if in production
        if os.getenv('ENVIRONMENT') == 'production':
            s3_bucket = os.getenv('RAW_S3_BUCKET')
            s3_prefix = os.getenv('RAW_S3_PREFIX', 'raw/loans')
            if s3_bucket:
                cmd.extend(['--prod', '--s3_bucket', s3_bucket, '--s3_prefix', s3_prefix])
        
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        print("âœ… Ingestion completed successfully")
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Ingestion failed: {e}")
        raise

def run_dbt_models(**context) -> bool:
    """Run dbt models (SILVER and GOLD layers)"""
    paths = get_project_paths()
    
    try:
        cmd = ["dbt", "run"]
        result = subprocess.run(cmd, cwd=str(paths["dbt_dir"]), capture_output=True, text=True, check=True)
        print("âœ… dbt run completed successfully")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ dbt run failed: {e}")
        raise

def validate_data_quality(**context) -> bool:
    """Additional data quality checks beyond dbt tests"""
    paths = get_project_paths()
    
    try:
        import duckdb
        con = duckdb.connect(str(paths["duckdb_path"]))
        
        # Check row counts
        raw_count = con.execute("SELECT COUNT(*) FROM raw.raw_loans").fetchone()[0]
        silver_count = con.execute("SELECT COUNT(*) FROM main_silver.stg_loans").fetchone()[0]
        gold_count = con.execute("SELECT COUNT(*) FROM main_gold.fact_loan").fetchone()[0]
        
        print(f"ğŸ“Š Data counts: RAW={raw_count}, SILVER={silver_count}, GOLD={gold_count}")
        
        # Basic validation
        if raw_count == 0:
            raise ValueError("RAW layer is empty")
        if silver_count == 0:
            raise ValueError("SILVER layer is empty")
        if gold_count == 0:
            raise ValueError("GOLD layer is empty")
        
        print("âœ… Data quality validation passed")
        return True
        
    except Exception as e:
        print(f"âŒ Data quality validation failed: {e}")
        raise
```

### âš™ï¸ ConfiguraciÃ³n

#### `airflow/utils/config.py`
```python
# DAG Configuration
DAG_CONFIG = {
    'dag_id': 'loan_data_pipeline',
    'description': 'Loan data pipeline: Excel â†’ RAW â†’ SILVER â†’ GOLD',
    'schedule': timedelta(days=1),
    'catchup': False,
    'tags': ['loan', 'dbt', 'data-pipeline'],
}

# Task Configuration
TASK_CONFIG = {
    'ingest': {
        'task_id': 'ingest_excel_to_raw',
        'description': 'Ingest Excel data to RAW layer (Parquet + DuckDB)',
    },
    'dbt_deps': {
        'task_id': 'dbt_deps',
        'description': 'Install dbt dependencies',
    },
    'dbt_run': {
        'task_id': 'dbt_run_models',
        'description': 'Run dbt models (SILVER and GOLD layers)',
    },
    'dbt_test': {
        'task_id': 'dbt_run_tests',
        'description': 'Run dbt tests for data quality validation',
    },
    'data_quality': {
        'task_id': 'validate_data_quality',
        'description': 'Additional data quality checks',
    },
    'dbt_docs': {
        'task_id': 'dbt_generate_docs',
        'description': 'Generate dbt documentation',
    },
}
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### ğŸ“‹ Prerrequisitos
- Python 3.13+
- Apache Airflow 3.0.4+
- dbt-core 1.10.9+
- DuckDB

### ğŸ”§ InstalaciÃ³n Local

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd data_engineer_challenge_local_duckdb
```

2. **Crear entorno virtual**
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# o
.venv\Scripts\activate     # Windows
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Configurar Airflow**
```bash
export AIRFLOW_HOME=$(pwd)/airflow
export PYTHONPATH=$PYTHONPATH:$(pwd)/airflow
airflow standalone
```

5. **Acceder a la interfaz web**
- URL: http://localhost:8080
- Usuario: `admin`
- ContraseÃ±a: `cvPPAEfuV7bSTP6s`

### ğŸ¯ EjecuciÃ³n del Pipeline

#### OpciÃ³n 1: EjecuciÃ³n Local
```bash
python scripts/run_pipeline.py
```

#### OpciÃ³n 2: EjecuciÃ³n con Airflow
1. Acceder a http://localhost:8080
2. Buscar el DAG `loan_data_pipeline`
3. Hacer clic en "Trigger DAG"

#### OpciÃ³n 3: Explorar Base de Datos
```bash
python scripts/explore_db.py
```

## ğŸ“Š Monitoreo y Logs

### ğŸ” Logs de Airflow
- **Web UI**: http://localhost:8080
- **Logs de tareas**: Disponibles en la interfaz web
- **Estado del DAG**: Monitoreo en tiempo real

### ğŸ“ˆ MÃ©tricas de Calidad
- **Tests dbt**: ValidaciÃ³n automÃ¡tica de datos
- **Checks personalizados**: ValidaciÃ³n adicional de conteos
- **DocumentaciÃ³n**: GeneraciÃ³n automÃ¡tica de docs

## ğŸ”§ Comandos Airflow

### ğŸ“Š GestiÃ³n de DAGs
```bash
# Listar DAGs
airflow dags list

# Ver informaciÃ³n del DAG
airflow dags show loan_data_pipeline

# Trigger manual del DAG
airflow dags trigger loan_data_pipeline

# Pausar/Reanudar DAG
airflow dags pause loan_data_pipeline
airflow dags unpause loan_data_pipeline
```

### ğŸ” Logs y Debugging
```bash
# Ver logs de una tarea especÃ­fica
airflow tasks logs loan_data_pipeline ingest_excel_to_raw latest

# Ver logs de una ejecuciÃ³n especÃ­fica
airflow dags backfill loan_data_pipeline --start-date 2024-01-01 --end-date 2024-01-02
```

### âš™ï¸ ConfiguraciÃ³n
```bash
# Configurar variables de entorno
airflow config set core pythonpath $(pwd)/airflow

# Ver configuraciÃ³n actual
airflow config get-value core pythonpath
```

## ğŸ¯ CaracterÃ­sticas del DAG

### âœ… **OrquestaciÃ³n Completa**
- EjecuciÃ³n secuencial de tareas
- Manejo de dependencias
- Retry automÃ¡tico en fallos

### âœ… **Monitoreo en Tiempo Real**
- Interfaz web intuitiva
- Logs detallados por tarea
- MÃ©tricas de ejecuciÃ³n

### âœ… **Flexibilidad**
- ConfiguraciÃ³n por variables de entorno
- Modo desarrollo y producciÃ³n
- IntegraciÃ³n con S3 opcional

### âœ… **Escalabilidad**
- Preparado para mÃºltiples ejecuciones
- ConfiguraciÃ³n de recursos
- Manejo de concurrencia

## ğŸš€ Optimizaciones

### ğŸ“Š **Performance**
- EjecuciÃ³n paralela donde es posible
- OptimizaciÃ³n de recursos
- Cache de dependencias

### ğŸ” **Debugging**
- Logs estructurados
- InformaciÃ³n de contexto
- Trazabilidad completa

### ğŸ›¡ï¸ **Robustez**
- Manejo de errores
- Retry automÃ¡tico
- Alertas configurables

## ğŸ”„ Variables de Entorno

### ğŸ”§ **Desarrollo**
```bash
export AIRFLOW_HOME=$(pwd)/airflow
export PYTHONPATH=$PYTHONPATH:$(pwd)/airflow
```

### ğŸš€ **ProducciÃ³n**
```bash
export ENVIRONMENT=production
export RAW_S3_BUCKET=my-raw-data-bucket
export RAW_S3_PREFIX=raw/loans
```

## ğŸ“ˆ MÃ©tricas de EjecuciÃ³n

### â±ï¸ **Tiempos Promedio**
- **Ingesta**: ~30 segundos
- **dbt deps**: ~5 segundos
- **dbt run**: ~10 segundos
- **dbt test**: ~5 segundos
- **Data quality**: ~2 segundos
- **dbt docs**: ~3 segundos
- **Total**: ~55 segundos

### ğŸ“Š **Tasa de Ã‰xito**
- **Tests pasados**: 100%
- **Tareas exitosas**: 100%
- **DAG completado**: 100%

---

*DocumentaciÃ³n Airflow - Pipeline de PrÃ©stamos v1.0*
