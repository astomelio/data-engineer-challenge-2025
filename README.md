# ğŸš€ Data Engineer Challenge - Pipeline de PrÃ©stamos

## ğŸ“‹ DescripciÃ³n General

Este proyecto implementa un **pipeline de datos completo** para procesar informaciÃ³n de prÃ©stamos desde un archivo Excel hasta un modelo dimensional optimizado para analytics. El pipeline sigue la arquitectura **RAW â†’ SILVER â†’ GOLD** y utiliza **Apache Airflow** para orquestaciÃ³n.

## ğŸ—ï¸ Arquitectura del Pipeline

### ğŸ“Š Capas de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LANDING       â”‚    â”‚     SILVER      â”‚    â”‚      GOLD       â”‚
â”‚   (RAW)         â”‚â”€â”€â”€â–¶â”‚   (Structured)  â”‚â”€â”€â”€â–¶â”‚   (Analytics)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ Excel Source  â”‚    â”‚ â€¢ Cleaned Data  â”‚    â”‚ â€¢ Fact Tables   â”‚
â”‚ â€¢ Parquet Files â”‚    â”‚ â€¢ Deduplication â”‚    â”‚ â€¢ Dimension     â”‚
â”‚ â€¢ Raw Tables    â”‚    â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Star Schema   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Flujo de Datos

1. **LANDING (RAW)**: Datos originales del Excel â†’ `raw.raw_loans` en DuckDB
2. **SILVER**: Datos limpios y estructurados â†’ `silver.silver_loans`
3. **GOLD**: Modelo dimensional para analytics â†’ `gold.fact_loan`, `gold.dim_*`

### âš ï¸ **Arquitectura Actual vs ProducciÃ³n**
- **Desarrollo (Actual)**: RAW almacenado en DuckDB local
- **ProducciÃ³n (AWS)**: RAW deberÃ­a almacenarse en S3 buckets
- **MigraciÃ³n**: Script incluye funcionalidad S3 para producciÃ³n

## ğŸ“ Estructura del Proyecto

```
data_engineer_challenge_local_duckdb/
â”œâ”€â”€ ğŸ“Š data/                          # Datos fuente
â”‚   â””â”€â”€ Data Engineer Challenge.xlsx  # Archivo Excel original
â”œâ”€â”€ ğŸ”§ ingest/                        # Scripts de ingesta
â”‚   â””â”€â”€ ingest_excel_to_duckdb.py    # Procesamiento inicial
â”œâ”€â”€ ğŸ¯ dbt/                          # Transformaciones dbt
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ silver/                  # Capa SILVER
â”‚   â”‚   â”‚   â””â”€â”€ silver_loans.sql     # Limpieza y deduplicaciÃ³n
â”‚   â”‚   â””â”€â”€ gold/                    # Capa GOLD
â”‚   â”‚       â”œâ”€â”€ dim_customer.sql     # DimensiÃ³n clientes
â”‚   â”‚       â”œâ”€â”€ dim_purpose.sql      # DimensiÃ³n propÃ³sitos
â”‚   â”‚       â””â”€â”€ fact_loan.sql        # Tabla de hechos
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ packages.yml
â”œâ”€â”€ â˜ï¸ airflow/                      # OrquestaciÃ³n Airflow
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ loan_pipeline_dag.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ pipeline_functions.py
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ ğŸ“œ scripts/                      # Scripts de utilidad
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â””â”€â”€ explore_db.py
â”œâ”€â”€ ğŸ“š docs/                         # DocumentaciÃ³n tÃ©cnica
â”‚   â”œâ”€â”€ dbt_documentation.md
â”‚   â”œâ”€â”€ airflow_documentation.md
â”‚   â””â”€â”€ pipeline_stages.md
â””â”€â”€ ğŸ“š README.md                     # Esta documentaciÃ³n
```

## ğŸš€ Inicio RÃ¡pido

### ğŸ“‹ Prerrequisitos
- Python 3.13+
- Apache Airflow 3.0.4+
- dbt-core 1.10.9+
- DuckDB

### ğŸ”§ InstalaciÃ³n

1. **Clonar y configurar**
```bash
git clone <repository-url>
cd data_engineer_challenge_local_duckdb
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
```

2. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

3. **Configurar Airflow**
```bash
export AIRFLOW_HOME=$(pwd)/airflow
export PYTHONPATH=$PYTHONPATH:$(pwd)/airflow
airflow standalone
```

### ğŸ¯ EjecuciÃ³n

#### OpciÃ³n 1: Pipeline Local
```bash
python scripts/run_pipeline.py
```

#### OpciÃ³n 2: Pipeline con Airflow
1. Acceder a http://localhost:8080 (admin/cvPPAEfuV7bSTP6s)
2. Buscar DAG `loan_data_pipeline`
3. Hacer clic en "Trigger DAG"

#### OpciÃ³n 3: Explorar Base de Datos
```bash
python scripts/explore_db.py
```

## ğŸ“Š Resultados del Pipeline

### ğŸ“ˆ EstadÃ­sticas de Datos
- **RAW**: 100,000 registros originales (`raw.raw_loans`)
- **SILVER**: 81,999 registros (18% de duplicados removidos) (`silver.silver_loans`)
- **GOLD**: 
  - Fact Table: 81,999 registros (`gold.fact_loan`)
  - Dim Customer: 81,999 clientes Ãºnicos (`gold.dim_customer`)
  - Dim Purpose: 16 propÃ³sitos Ãºnicos (`gold.dim_purpose`)

### ğŸ¯ DistribuciÃ³n de Estados
- **Fully Paid**: 59,360 (72.4%)
- **Charged Off**: 22,639 (27.6%)

### ğŸ“‹ PropÃ³sitos Principales
- **Debt Consolidation**: 64,907 (79.2%)
- **Home Improvements**: 4,795 (5.8%)
- **Other**: 7,238 (8.8%)

## ğŸ“š DocumentaciÃ³n TÃ©cnica

Para informaciÃ³n detallada sobre cada componente:

- **[DocumentaciÃ³n dbt](docs/dbt_documentation.md)** - Transformaciones y modelos
- **[DocumentaciÃ³n Airflow](docs/airflow_documentation.md)** - OrquestaciÃ³n y configuraciÃ³n
- **[Etapas del Pipeline](docs/pipeline_stages.md)** - Flujo completo y mÃ©tricas

## ğŸ”§ Comandos Ãštiles

### ğŸ“Š Monitoreo
```bash
# Ver logs de Airflow
tail -f airflow/logs/dag_id/task_id/run_id/task_id.log

# Ejecutar dbt con debug
cd dbt && dbt run --log-level debug
```

### ğŸ§¹ Mantenimiento
```bash
# Limpiar dbt
cd dbt && dbt clean

# Reprocesar desde cero
python scripts/run_pipeline.py
```

## ğŸ¯ CaracterÃ­sticas Principales

- âœ… **Escalabilidad**: Preparado para S3 en producciÃ³n (RAW layer)
- âœ… **Calidad**: Tests automÃ¡ticos y validaciÃ³n
- âœ… **DocumentaciÃ³n**: GeneraciÃ³n automÃ¡tica de docs
- âœ… **Monitoreo**: Airflow para orquestaciÃ³n
- âœ… **Flexibilidad**: MÃºltiples opciones de ejecuciÃ³n

## ğŸš€ PrÃ³ximos Pasos

- Implementar alertas de calidad
- Agregar mÃ¡s tests de negocio
- Optimizar para volÃºmenes mayores
- Implementar versionado de datos

---

*Pipeline de PrÃ©stamos v1.0 - DocumentaciÃ³n simplificada*

