# ğŸš€ Data Engineer Challenge - Loan Pipeline

## ğŸ“‹ Overview

This project implements a **complete data pipeline** to process loan information from an Excel file to an optimized dimensional model for analytics. The pipeline follows the **RAW â†’ SILVER â†’ GOLD** architecture and uses **Apache Airflow** for orchestration.

## ğŸ—ï¸ Pipeline Architecture

### ğŸ“Š Data Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LANDING       â”‚    â”‚     SILVER      â”‚    â”‚      GOLD       â”‚
â”‚   (RAW)         â”‚â”€â”€â”€â–¶â”‚   (Structured)  â”‚â”€â”€â”€â–¶â”‚   (Analytics)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ Excel Source  â”‚    â”‚ â€¢ Cleaned Data  â”‚    â”‚ â€¢ Fact Tables   â”‚
â”‚ â€¢ Parquet Files â”‚    â”‚ â€¢ Deduplication â”‚    â”‚ â€¢ Dimensions    â”‚
â”‚ â€¢ Raw Tables    â”‚    â”‚ â€¢ Validation    â”‚    â”‚ â€¢ Star Schema   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Data Flow

1. **LANDING (RAW)**: Original Excel data â†’ `raw.raw_loans` in DuckDB
2. **SILVER**: Cleaned and structured data â†’ `silver.silver_loans`
3. **GOLD**: Dimensional model for analytics â†’ `gold.fact_loan`, `gold.dim_*`

### âš ï¸ **Current vs Production Architecture**
- **Development (Current)**: RAW stored in local DuckDB
- **Production (AWS)**: RAW should be stored in S3 buckets
- **Migration**: Script includes S3 functionality for production

## ğŸ“ Project Structure

```
data_engineer_challenge_local_duckdb/
â”œâ”€â”€ ğŸ“Š data/                          # Source data
â”‚   â””â”€â”€ Data Engineer Challenge.xlsx  # Original Excel file
â”œâ”€â”€ ğŸ”§ ingest/                        # Ingestion scripts
â”‚   â””â”€â”€ ingest_excel_to_duckdb.py    # Initial processing
â”œâ”€â”€ ğŸ¯ dbt/                          # dbt transformations
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ silver/                  # SILVER layer
â”‚   â”‚   â”‚   â””â”€â”€ silver_loans.sql     # Cleaning and deduplication
â”‚   â”‚   â””â”€â”€ gold/                    # GOLD layer
â”‚   â”‚       â”œâ”€â”€ dim_customer.sql     # Customer dimension
â”‚   â”‚       â”œâ”€â”€ dim_purpose.sql      # Purpose dimension
â”‚   â”‚       â””â”€â”€ fact_loan.sql        # Fact table
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ packages.yml
â”œâ”€â”€ â˜ï¸ airflow/                      # Airflow orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ loan_pipeline_dag.py
â”‚   â”‚   â””â”€â”€ loan_pipeline_incremental_dag.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ pipeline_functions.py
â”‚       â””â”€â”€ config.py
â”œâ”€â”€ ğŸ“œ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â”œâ”€â”€ explore_db.py
â”‚   â””â”€â”€ quick_stats.py
â”œâ”€â”€ ğŸ“š docs/                         # Technical documentation
â”‚   â”œâ”€â”€ COMPLETE_DOCUMENTATION.md
â”‚   â”œâ”€â”€ NULL_MEANING_ANALYSIS.md
â”‚   â””â”€â”€ dbt_documentation.md
â””â”€â”€ ğŸ“š README.md                     # This documentation
```

## ğŸš€ Quick Start

### ğŸ“‹ Prerequisites
- Python 3.13+
- Apache Airflow 3.0.4+
- dbt-core 1.10.9+
- DuckDB

### ğŸ”§ Installation

1. **Clone and setup**
```bash
git clone <repository-url>
cd data_engineer_challenge_local_duckdb
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Configure Airflow**
```bash
export AIRFLOW_HOME=$(pwd)/airflow
export PYTHONPATH=$PYTHONPATH:$(pwd)/airflow
airflow standalone
```

### ğŸ¯ Execution

#### Option 1: Local Pipeline
```bash
python scripts/run_pipeline.py
```

#### Option 2: Pipeline with Airflow
1. Access http://localhost:8080 (admin/cvPPAEfuV7bSTP6s)
2. Find DAG `loan_data_pipeline`
3. Click "Trigger DAG"

#### Option 3: Explore Database
```bash
python scripts/explore_db.py
```

#### Option 4: Quick Statistics
```bash
python scripts/quick_stats.py
```

## ğŸ“Š Pipeline Results

### ğŸ“ˆ Data Statistics
- **RAW**: 100,000 original records (`raw.raw_loans`)
- **SILVER**: 81,999 records (18% duplicates removed) (`silver.silver_loans`)
- **GOLD**: 
  - Fact Table: 81,999 records (`gold.fact_loan`)
  - Dim Customer: 81,999 unique customers (`gold.dim_customer`)
  - Dim Purpose: 16 unique purposes (`gold.dim_purpose`)

### ğŸ¯ Status Distribution
- **Fully Paid**: 59,360 (72.4%)
- **Charged Off**: 22,639 (27.6%)

### ğŸ“‹ Main Purposes
- **Debt Consolidation**: 64,907 (79.2%)
- **Home Improvements**: 4,795 (5.8%)
- **Other**: 7,238 (8.8%)

## ğŸ“š Technical Documentation

For detailed information about each component:

- **[Complete Documentation](docs/COMPLETE_DOCUMENTATION.md)** - Full technical overview
- **[NULL Analysis](docs/NULL_MEANING_ANALYSIS.md)** - Business meaning of NULL values
- **[dbt Documentation](docs/dbt_documentation.md)** - Transformations and models

## ğŸ”§ Useful Commands

### ğŸ“Š Monitoring
```bash
# View Airflow logs
tail -f airflow/logs/dag_id/task_id/run_id/task_id.log

# Run dbt with debug
cd dbt && dbt run --log-level debug

# List your project DAGs
bash airflow/scripts/my_dags.sh
```

### ğŸ§¹ Maintenance
```bash
# Clean dbt
cd dbt && dbt clean

# Reprocess from scratch
python scripts/run_pipeline.py

# Generate dbt docs
cd dbt && dbt docs generate && dbt docs serve
```

## ğŸ¯ Key Features

- âœ… **Scalability**: Ready for S3 in production (RAW layer)
- âœ… **Data Quality**: Automatic tests and validation
- âœ… **Documentation**: Auto-generated documentation
- âœ… **Monitoring**: Airflow for orchestration
- âœ… **Flexibility**: Multiple execution options
- âœ… **Incremental Processing**: Support for both full refresh and incremental modes

## ğŸš€ Next Steps

- Implement data quality alerts
- Add more business tests
- Optimize for larger volumes
- Implement data versioning
- Add data lineage visualization

---

*Loan Pipeline v1.0 - Professional documentation for enterprise presentation*